# Copyright (c) Forward Email LLC
# SPDX-License-Identifier: BUSL-1.1

---
- name: Import security playbook
  import_playbook: security.yml
- name: Import SSH keys playbook
  import_playbook: ssh-keys.yml

- hosts: mongo
  name: Certificate Paths
  vars_prompt:
    - name: input_key
      prompt: "Enter path to certificate private key file on local machine (e.g. /path/to/.ssl-key) [REQUIRED]"
      private: false
    - name: input_cert
      prompt: "Enter path to certificate full chain/certificate file on local machine (e.g. /path/to/.ssl-cert) [REQUIRED]"
      private: false
    - name: input_ca
      prompt: "Enter path to certificate CA bundle file on local machine (e.g. /path/to/.ssl-ca) [REQUIRED]"
      private: false
  tasks:
    - name: Fail if key path is empty
      fail:
        msg: "Certificate private key path is required. Please re-run the playbook and provide a valid path."
      when: (input_key is not defined) or (input_key | length == 0)

    - name: Fail if cert path is empty
      fail:
        msg: "Certificate file path is required. Please re-run the playbook and provide a valid path."
      when: (input_cert is not defined) or (input_cert | length == 0)

    - name: Fail if CA path is empty
      fail:
        msg: "CA bundle file path is required. Please re-run the playbook and provide a valid path."
      when: (input_ca is not defined) or (input_ca | length == 0)

    - name: Check if key file exists locally
      local_action: stat path={{ input_key }}
      become: false
      register: local_key_file

    - name: Fail when local key file does not exist
      fail:
        msg: "key file does not exist: {{ input_key }}"
      when: not local_key_file.stat.exists

    - name: Check if cert file exists locally
      local_action: stat path={{ input_cert }}
      become: false
      register: local_cert_file

    - name: Fail when local cert file does not exist
      fail:
        msg: "cert file does not exist: {{ input_cert }}"
      when: not local_cert_file.stat.exists

    - name: Check if CA file exists locally
      local_action: stat path={{ input_ca }}
      become: false
      register: local_ca_file

    - name: Fail when local CA file does not exist
      fail:
        msg: "CA file does not exist: {{ input_ca }}"
      when: not local_ca_file.stat.exists

    - name: Store certificate paths as facts
      set_fact:
        ssl_key_path: "{{ input_key }}"
        ssl_cert_path: "{{ input_cert }}"
        ssl_ca_path: "{{ input_ca }}"

- hosts: mongo
  name: Hostname
  become: true
  become_user: root
  tasks:
    - name: Validate required environment variables
      assert:
        that:
          - lookup('env', 'MONGO_HOST') != ''
          - lookup('env', 'MONGO_PORT') != ''
          - lookup('env', 'AWS_ACCESS_KEY_ID') != ''
          - lookup('env', 'AWS_SECRET_ACCESS_KEY') != ''
          - lookup('env', 'AWS_ENDPOINT_URL') != ''
          - lookup('env', 'BACKUP_SECRET') != ''
        fail_msg: |
          Required environment variables are missing for MongoDB playbook.
          Please ensure the following variables are set in your .env file:
          - MONGO_HOST
          - MONGO_PORT
          - SSL_CERT_PATH
          - SSL_KEY_PATH
          - SSL_CA_PATH
          - AWS_ACCESS_KEY_ID
          - AWS_SECRET_ACCESS_KEY
          - AWS_ENDPOINT_URL
          - BACKUP_SECRET
        quiet: false

    - name: Set hostname
      hostname:
        name: "{{ lookup('env', 'MONGO_HOST') }}"

- hosts: mongo
  name: MongoDB
  become: true
  become_user: root
  handlers:
    - name: Reload UFW
      ufw:
        state: reloaded
    # NOTE: MongoDB restart removed - must be done manually
    # See post-deployment instructions for manual restart procedure
  tasks:
    # MongoDB-specific sysctl optimizations
    # Note: THP, ulimits, and base sysctl settings are inherited from security.yml
    - name: Configure MongoDB-specific sysctl parameters
      ansible.builtin.sysctl:
        name: "{{ item.name }}"
        value: "{{ item.value }}"
        state: present
        reload: true
        sysctl_file: /etc/sysctl.d/99-mongodb.conf
      loop:
        - { name: 'vm.dirty_background_ratio', value: '10' }
        - { name: 'vm.dirty_ratio', value: '20' }
        - { name: 'net.core.somaxconn', value: '4096' }
        - { name: 'net.ipv4.tcp_max_syn_backlog', value: '4096' }
        - { name: 'net.ipv4.tcp_fin_timeout', value: '30' }
        - { name: 'net.ipv4.tcp_keepalive_intvl', value: '30' }
        - { name: 'net.ipv4.tcp_keepalive_time', value: '120' }

    # Configure swap for MongoDB stability
    # MongoDB requires swap to prevent OOM crashes during high memory usage
    # Best practice: swap file size = total system RAM
    - name: Get total system memory in MB
      shell: free -m | awk '/^Mem:/{print $2}'
      register: total_memory_mb
      changed_when: false

    - name: Check if swap file exists
      stat:
        path: /swapfile
      register: swapfile_stat

    - name: Check current swap size
      shell: swapon --show=SIZE --noheadings --bytes | head -1
      register: current_swap_bytes
      changed_when: false
      failed_when: false

    - name: Calculate required swap size in bytes
      set_fact:
        required_swap_mb: "{{ total_memory_mb.stdout | int }}"
        required_swap_bytes: "{{ (total_memory_mb.stdout | int) * 1024 * 1024 }}"

    - name: Disable existing swap if size doesn't match
      command: swapoff /swapfile
      when:
        - swapfile_stat.stat.exists
        - current_swap_bytes.stdout != ""
        - current_swap_bytes.stdout | int != required_swap_bytes | int
      ignore_errors: true

    - name: Remove old swap file if size doesn't match
      file:
        path: /swapfile
        state: absent
      when:
        - swapfile_stat.stat.exists
        - current_swap_bytes.stdout != ""
        - current_swap_bytes.stdout | int != required_swap_bytes | int

    - name: Create swap file with size equal to RAM
      command: fallocate -l {{ required_swap_mb }}M /swapfile
      args:
        creates: /swapfile

    - name: Set swap file permissions
      file:
        path: /swapfile
        mode: '0600'
        owner: root
        group: root

    - name: Format swap file
      command: mkswap /swapfile
      when: not swapfile_stat.stat.exists or (current_swap_bytes.stdout | int != required_swap_bytes | int)
      register: mkswap_result
      changed_when: "'Setting up swapspace' in mkswap_result.stdout"

    - name: Enable swap file
      command: swapon /swapfile
      when: mkswap_result is changed

    - name: Add swap to /etc/fstab
      lineinfile:
        path: /etc/fstab
        line: '/swapfile none swap sw 0 0'
        state: present
        regexp: '^/swapfile'

    - name: Set swappiness for MongoDB (minimize swapping but allow it for safety)
      ansible.builtin.sysctl:
        name: vm.swappiness
        value: '1'
        state: present
        reload: true
        sysctl_file: /etc/sysctl.d/99-mongodb.conf

    # Install MongoDB using custom installation (no Galaxy role)
    # WARNING: DO NOT upgrade to MongoDB 7.0 or 8.0 due to severe performance regressions
    # MongoDB 7.0 has been reported to be up to 25x slower than 6.0 for index-based queries
    # MongoDB 8.0's "36% faster" claim is compared to 7.0, NOT 6.0
    # Root causes: Slot-Based Execution engine issues and WiredTiger ticket reduction
    # Forward Email's query patterns (aggregations, index lookups, regex) are directly affected
    # References:
    # - https://www.mongodb.com/community/forums/t/performance-drop-after-upgrade-6-0-10-7-0-1/246712
    # - https://revisit.tech/blog/mongodb-8-0-performanace-is-36-percent-higher-but-there-is-a-catch/
    # - https://dev.to/manoj_from_revisit_dot_tech/mongodb-80-performance-is-36-higher-nope-its-not--35jk
    
    - name: Add MongoDB GPG key
      apt_key:
        url: https://www.mongodb.org/static/pgp/server-6.0.asc
        state: present

    - name: Add MongoDB repository
      apt_repository:
        repo: "deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu {{ ansible_distribution_release }}/mongodb-org/6.0 multiverse"
        state: present
        filename: mongodb-org-6.0

    - name: Install MongoDB 6.0.18
      apt:
        name:
          - mongodb-org=6.0.18
          - mongodb-org-database=6.0.18
          - mongodb-org-server=6.0.18
          - mongodb-org-mongos=6.0.18
          - mongodb-org-tools=6.0.18
        state: present
        update_cache: yes

    - name: Hold MongoDB packages to prevent accidental upgrades
      dpkg_selections:
        name: "{{ item }}"
        selection: hold
      loop:
        - mongodb-org
        - mongodb-org-database
        - mongodb-org-server
        - mongodb-org-mongos
        - mongodb-org-tools

    - name: Ensure MongoDB data directory exists
      file:
        path: /var/lib/mongodb
        state: directory
        owner: mongodb
        group: mongodb
        mode: '0755'

    - name: Ensure MongoDB log directory exists
      file:
        path: /var/log/mongodb
        state: directory
        owner: mongodb
        group: mongodb
        mode: '0755'

    - name: Enable and start mongod service
      systemd:
        name: mongod
        enabled: yes
        state: started
        daemon_reload: yes

    # Add failure notification to mongod service
    - name: Create mongod service override directory
      file:
        path: /etc/systemd/system/mongod.service.d
        state: directory
        owner: root
        group: root
        mode: '0755'

    - name: Add OnFailure notification to mongod service
      copy:
        dest: /etc/systemd/system/mongod.service.d/failure-notification.conf
        content: |
          # WARNING: This file is managed by Ansible
          # Manual changes will be overwritten on next deployment
          [Unit]
          OnFailure=failure-notification@%n.service
        owner: root
        group: root
        mode: '0644'

    - name: Reload systemd daemon for mongod override
      systemd:
        daemon_reload: yes

    # Configure MongoDB for TLS/SSL
    - name: Ensure MongoDB TLS/SSL configuration directory exists
      file:
        path: /etc/mongodb/ssl
        state: directory
        owner: mongodb
        group: mongodb
        mode: "0750"

    # Copy SSL certificates directly from local machine to MongoDB directory
    - name: Copy SSL certificate to MongoDB directory
      copy:
        src: "{{ hostvars[inventory_hostname]['ssl_cert_path'] }}"
        dest: /etc/mongodb/ssl/mongodb.crt
        owner: mongodb
        group: mongodb
        mode: "0400"

    - name: Copy SSL private key to MongoDB directory
      copy:
        src: "{{ hostvars[inventory_hostname]['ssl_key_path'] }}"
        dest: /etc/mongodb/ssl/mongodb.key
        owner: mongodb
        group: mongodb
        mode: "0400"

    - name: Copy CA certificate to MongoDB directory
      copy:
        src: "{{ hostvars[inventory_hostname]['ssl_ca_path'] }}"
        dest: /etc/mongodb/ssl/ca.pem
        owner: mongodb
        group: mongodb
        mode: "0400"

    - name: Create MongoDB TLS/SSL certificate PEM file (cert + key)
      shell: |
        cat /etc/mongodb/ssl/mongodb.crt /etc/mongodb/ssl/mongodb.key > /etc/mongodb/ssl/mongodb.pem
        chown mongodb:mongodb /etc/mongodb/ssl/mongodb.pem
        chmod 0400 /etc/mongodb/ssl/mongodb.pem
      args:
        creates: /etc/mongodb/ssl/mongodb.pem

    - name: Configure MongoDB TLS/SSL in mongod.conf
      blockinfile:
        path: /etc/mongod.conf
        marker: "# {mark} ANSIBLE MANAGED BLOCK - TLS/SSL"
        block: |
          net:
            tls:
              mode: requireTLS
              certificateKeyFile: /etc/mongodb/ssl/mongodb.pem
              CAFile: /etc/mongodb/ssl/ca.pem

    # UFW Configuration
    - name: Enable ufw
      ufw:
        state: enabled
        policy: deny
        direction: incoming

    - name: Limit ufw ssh
      ufw:
        rule: limit
        port: 22
        proto: tcp

    - name: Allow ssh
      ufw:
        rule: allow
        port: 22
        proto: tcp

    # Install UFW whitelist update script
    # Note: Initial IP whitelist will be populated when the script runs for the first time
    - name: Create UFW whitelist update script
      copy:
        dest: /usr/local/bin/update-mongo-ufw-whitelist.sh
        mode: "0755"
        content: |
          #!/bin/bash
          set -e

          # Configuration
          MONGO_PORT="${MONGO_PORT:-27017}"
          IP_LIST_URL="https://forwardemail.net/ips/v4.txt?comments=false"
          COMMENT="Auto-whitelist MongoDB"
          SERVICE_NAME="MongoDB"
          POSTFIX_RCPTS="${POSTFIX_RCPTS:-security@forwardemail.net}"

          echo "[$(date -Iseconds)] Fetching IP whitelist..."
          
          # Fetch with retries (3 attempts, 10 second timeout each)
          for attempt in {1..3}; do
            NEW_IPS=$(curl -s --max-time 10 "$IP_LIST_URL" | grep -E '^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$' | sort -u)
            
            if [ -n "$NEW_IPS" ]; then
              echo "[$(date -Iseconds)] Successfully fetched $(echo "$NEW_IPS" | wc -l) IPs"
              break
            fi
            
            echo "[$(date -Iseconds)] Fetch attempt $attempt failed, retrying..."
            sleep 5
          done

          if [ -z "$NEW_IPS" ]; then
            echo "[$(date -Iseconds)] ERROR: Failed to fetch IP list after 3 attempts"
            echo "[$(date -Iseconds)] Keeping existing UFW rules unchanged"
            exit 0  # Exit gracefully - don't break existing rules
          fi

          # Get current IPs from UFW for the CORRECT port
          CURRENT_IPS=$(ufw status | grep "${MONGO_PORT}/tcp" | grep "$COMMENT" | awk '{print $3}' | grep -E '^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$' | sort -u)

          # Find orphaned rules with the comment but WRONG port (from old deploys)
          ORPHANED_RULES=$(ufw status numbered | grep "$COMMENT" | grep -v "${MONGO_PORT}/tcp" | grep -oP '^\[\s*\K[0-9]+' | sort -rn)

          # Remove orphaned rules first (delete from highest to lowest)
          ORPHANED_COUNT=0
          if [ -n "$ORPHANED_RULES" ]; then
            echo "[$(date -Iseconds)] Found orphaned rules with wrong port - cleaning up..."
            for rule_num in $ORPHANED_RULES; do
              echo "y" | ufw delete "$rule_num" > /dev/null 2>&1
              ORPHANED_COUNT=$((ORPHANED_COUNT + 1))
            done
            echo "[$(date -Iseconds)] Removed $ORPHANED_COUNT orphaned rules"
          fi

          # Calculate diff (only for correct port)
          TO_ADD=$(comm -13 <(echo "$CURRENT_IPS") <(echo "$NEW_IPS"))
          TO_REMOVE=$(comm -23 <(echo "$CURRENT_IPS") <(echo "$NEW_IPS"))

          # Check if changes needed
          if [ -z "$TO_ADD" ] && [ -z "$TO_REMOVE" ] && [ $ORPHANED_COUNT -eq 0 ]; then
            echo "[$(date -Iseconds)] No changes needed - sending summary email"
            CHANGES_MADE=false
          else
            echo "[$(date -Iseconds)] Changes detected - updating UFW rules..."
            CHANGES_MADE=true
          fi

          # Apply changes if needed
          REMOVED_COUNT=0
          ADDED_COUNT=0
          
          if [ "$CHANGES_MADE" = true ]; then
            # Remove old IPs (delete from highest rule number to lowest)
            if [ -n "$TO_REMOVE" ]; then
              for ip in $TO_REMOVE; do
                RULE_NUMS=$(ufw status numbered | grep "${MONGO_PORT}/tcp" | grep "$ip" | grep "$COMMENT" | grep -oP '^\[\s*\K[0-9]+' | sort -rn)
                for rule_num in $RULE_NUMS; do
                  echo "y" | ufw delete "$rule_num" > /dev/null 2>&1
                  REMOVED_COUNT=$((REMOVED_COUNT + 1))
                done
              done
            fi

            # Add new IPs
            if [ -n "$TO_ADD" ]; then
              for ip in $TO_ADD; do
                ufw allow from "$ip" to any port "$MONGO_PORT" proto tcp comment "$COMMENT" > /dev/null 2>&1
                ADDED_COUNT=$((ADDED_COUNT + 1))
              done
            fi

            # Note: No UFW reload needed - rules are applied immediately
            # Reloading would cause brief connection drops
          fi

          # Build email report
          TOTAL_IPS=$(echo "$NEW_IPS" | wc -l)
          ADDED_LIST=$(echo "$TO_ADD" | sed 's/^/  - /' | head -20)
          REMOVED_LIST=$(echo "$TO_REMOVE" | sed 's/^/  - /' | head -20)
          
          if [ $(echo "$TO_ADD" | wc -l) -gt 20 ]; then
            ADDED_LIST="$ADDED_LIST\n  ... and $(($(echo "$TO_ADD" | wc -l) - 20)) more"
          fi
          
          if [ $(echo "$TO_REMOVE" | wc -l) -gt 20 ]; then
            REMOVED_LIST="$REMOVED_LIST\n  ... and $(($(echo "$TO_REMOVE" | wc -l) - 20)) more"
          fi

          # Build status message
          if [ "$CHANGES_MADE" = true ]; then
            STATUS="âœ“ Changes Applied"
          else
            STATUS="âœ“ No Changes Needed"
          fi

          REPORT="UFW Whitelist Status for $SERVICE_NAME
          Timestamp: $(date -Iseconds)
          Status: $STATUS
          
          Added IPs ($ADDED_COUNT):
          $ADDED_LIST
          
          Removed IPs ($REMOVED_COUNT):
          $REMOVED_LIST
          
          Orphaned Rules Cleaned ($ORPHANED_COUNT):
            Rules with wrong port (from old deploys)
          
          Summary:
            - Total added: $ADDED_COUNT
            - Total removed: $REMOVED_COUNT
            - Orphaned cleaned: $ORPHANED_COUNT
            - Current total: $TOTAL_IPS IPs whitelisted
          
          Server: $(hostname)
          Port: $MONGO_PORT/tcp
          
          View logs: sudo journalctl -u mongo-ufw-whitelist-update.service -n 50"

          # Send email report (always, even if no changes)
          if [ "$CHANGES_MADE" = true ]; then
            SUBJECT="[UFW] Whitelist Updated: $SERVICE_NAME (Port $MONGO_PORT)"
          else
            SUBJECT="[UFW] Whitelist Status: $SERVICE_NAME (Port $MONGO_PORT) - No Changes"
          fi
          
          /usr/local/bin/send-rate-limited-email.sh \
            "ufw-whitelist-mongo" \
            "$SUBJECT" \
            "$REPORT"

          echo "[$(date -Iseconds)] UFW whitelist updated (added: $ADDED_COUNT, removed: $REMOVED_COUNT)"

    # Create systemd timer for UFW whitelist updates
    - name: Create UFW whitelist update systemd service
      copy:
        dest: /etc/systemd/system/mongo-ufw-whitelist-update.service
        content: |
          # WARNING: This file is managed by Ansible
          # Manual changes will be overwritten on next deployment
          [Unit]
          Description=Update MongoDB UFW IP Whitelist
          OnFailure=failure-notification@%n.service

          [Service]
          Type=oneshot
          RemainAfterExit=no
          ExecStart=/usr/local/bin/update-mongo-ufw-whitelist.sh

    - name: Create UFW whitelist update systemd timer
      copy:
        dest: /etc/systemd/system/mongo-ufw-whitelist-update.timer
        content: |
          # WARNING: This file is managed by Ansible
          # Manual changes will be overwritten on next deployment
          [Unit]
          Description=Update MongoDB UFW IP Whitelist Every 10 Minutes

          [Timer]
          OnBootSec=5min
          OnUnitActiveSec=10min

          [Install]
          WantedBy=timers.target

    - name: Enable and start UFW whitelist update timer
      systemd:
        daemon_reload: true
        name: mongo-ufw-whitelist-update.timer
        enabled: true
        state: started

       # Install AWS CLI and GPG dependencies
    - name: Install AWS CLI and GPG dependencies
      apt:
        name:
          - python3-pip
          - unzip
          - gnupg
        update_cache: true

    - name: Install AWS CLI
      shell: |
        if ! command -v aws &> /dev/null; then
          curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "/tmp/awscliv2.zip"
          unzip -q /tmp/awscliv2.zip -d /tmp
          /tmp/aws/install
          rm -rf /tmp/aws /tmp/awscliv2.zip
        fi
      args:
        creates: /usr/local/bin/aws

    # Create MongoDB backup script
    - name: Create MongoDB backup script
      copy:
        dest: /usr/local/bin/backup-mongodb.sh
        mode: "0755"
        content: |
          #!/bin/bash
          set -e
          exec 2>&1  # Redirect stderr to stdout for journal logging

          echo "Starting MongoDB backup at $(date -u +%Y-%m-%dT%H:%M:%SZ)"

          # Environment variables (set via systemd service or environment)
          AWS_ACCESS_KEY_ID="${AWS_ACCESS_KEY_ID}"
          AWS_SECRET_ACCESS_KEY="${AWS_SECRET_ACCESS_KEY}"
          AWS_ENDPOINT_URL="${AWS_ENDPOINT_URL}"
          AWS_DEFAULT_REGION="${AWS_DEFAULT_REGION:-auto}"
          BACKUP_BUCKET="${MONGO_BACKUP_BUCKET:-forwardemail-backups}"
          BACKUP_SECRET="${BACKUP_SECRET}"
          MONGO_PORT="${MONGO_PORT:-27017}"

          # Check if BACKUP_SECRET is set
          if [ -z "$BACKUP_SECRET" ]; then
            ERROR_MSG="BACKUP_SECRET environment variable is not set"
            echo "ERROR: $ERROR_MSG"
            /usr/local/bin/send-rate-limited-email.sh "mongodb-backup" "[ERROR] MongoDB Backup Failed on $(hostname)" "$ERROR_MSG"
            exit 1
          fi

          # Timestamp and path (flat structure - no folder nesting)
          BACKUP_NAME="mongo-backup-$(date -u +%Y-%m-%dT%H:%M:%SZ).gz.gpg"
          S3_PATH="s3://${BACKUP_BUCKET}/mongo/${BACKUP_NAME}"

          echo "Backup destination: ${S3_PATH}"

          # Check if bucket exists, create if not
          echo "Checking if S3 bucket exists: ${BACKUP_BUCKET}"
          if ! aws s3 ls "s3://${BACKUP_BUCKET}" --endpoint-url="${AWS_ENDPOINT_URL}" >/dev/null 2>&1; then
            echo "Bucket does not exist, creating: ${BACKUP_BUCKET}"
            if ! aws s3 mb "s3://${BACKUP_BUCKET}" --endpoint-url="${AWS_ENDPOINT_URL}" 2>&1; then
              ERROR_MSG="Failed to create S3 bucket ${BACKUP_BUCKET} at $(date -u +%Y-%m-%dT%H:%M:%SZ)"
              echo "ERROR: $ERROR_MSG"
              /usr/local/bin/send-rate-limited-email.sh "mongodb-backup" "[ERROR] MongoDB Backup Failed on $(hostname)" "$ERROR_MSG"
              exit 1
            fi
            echo "Bucket created successfully: ${BACKUP_BUCKET}"

            # Harden bucket security
            echo "Applying security hardening to bucket..."

            # Block public access (R2 specific)
            if aws s3api put-public-access-block \
              --bucket "${BACKUP_BUCKET}" \
              --public-access-block-configuration "BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true" \
              --endpoint-url="${AWS_ENDPOINT_URL}" 2>/dev/null; then
              echo "âœ“ Public access blocked"
            else
              echo "âš  Could not set public access block (may not be supported by R2)"
            fi

            # Set private ACL
            if aws s3api put-bucket-acl \
              --bucket "${BACKUP_BUCKET}" \
              --acl private \
              --endpoint-url="${AWS_ENDPOINT_URL}" 2>/dev/null; then
              echo "âœ“ Bucket ACL set to private"
            else
              echo "âš  Could not set bucket ACL (may not be supported by R2)"
            fi

            echo "Bucket security hardening completed"
          else
            echo "Bucket exists: ${BACKUP_BUCKET}"
          fi

          # Perform backup, encrypt with GPG, and stream directly to S3
          echo "Starting mongodump on localhost:${MONGO_PORT}..."
          if ! mongodump --host=localhost --port="${MONGO_PORT}" --oplog --archive --gzip 2>&1 | \
            gpg --symmetric --cipher-algo AES256 --batch --yes --passphrase "$BACKUP_SECRET" | \
            aws s3 cp - "${S3_PATH}" --endpoint-url="${AWS_ENDPOINT_URL}" 2>&1; then
            ERROR_MSG="Failed to backup/encrypt/upload MongoDB at $(date -u +%Y-%m-%dT%H:%M:%SZ). Check MongoDB connection and AWS credentials."
            echo "ERROR: $ERROR_MSG"
            /usr/local/bin/send-rate-limited-email.sh "mongodb-backup" "[ERROR] MongoDB Backup Failed on $(hostname)" "$ERROR_MSG"
            exit 1
          fi

          echo "MongoDB encrypted backup completed: ${S3_PATH}"
          echo "Backup finished at $(date -u +%Y-%m-%dT%H:%M:%SZ)"

    # Create MongoDB backup cleanup script
    - name: Create MongoDB backup cleanup script
      copy:
        dest: /usr/local/bin/cleanup-mongodb-backups.sh
        mode: "0755"
        content: |
          #!/bin/bash
          set -e
          exec 2>&1  # Redirect stderr to stdout for journal logging

          echo "Starting MongoDB backup cleanup at $(date -u +%Y-%m-%dT%H:%M:%SZ)"

          # Environment variables
          AWS_ACCESS_KEY_ID="${AWS_ACCESS_KEY_ID}"
          AWS_SECRET_ACCESS_KEY="${AWS_SECRET_ACCESS_KEY}"
          AWS_ENDPOINT_URL="${AWS_ENDPOINT_URL}"
          AWS_DEFAULT_REGION="${AWS_DEFAULT_REGION:-auto}"
          BACKUP_BUCKET="${MONGO_BACKUP_BUCKET:-forwardemail-backups}"

          # Counters for reporting
          DELETED_OLD=0
          DELETED_DAILY=0
          KEPT_RECENT=0
          ERRORS=0

          # Calculate dates
          THIRTY_DAYS_AGO=$(date -d "30 days ago" +%s)
          SEVEN_DAYS_AGO=$(date -d "7 days ago" +%s)

          echo "Retention policy: Keep all backups < 7 days, one per day for 8-30 days, delete > 30 days"
          echo "Checking bucket: s3://${BACKUP_BUCKET}/mongo/"

          # List all backups and process
          if ! BACKUP_LIST=$(aws s3 ls s3://${BACKUP_BUCKET}/mongo/ --recursive --endpoint-url="${AWS_ENDPOINT_URL}" 2>&1); then
            ERROR_MSG="Failed to list backups in s3://${BACKUP_BUCKET}/mongo/ at $(date -u +%Y-%m-%dT%H:%M:%SZ). Error: ${BACKUP_LIST}"
            echo "ERROR: $ERROR_MSG"
            /usr/local/bin/send-rate-limited-email.sh "mongo-cleanup" "[ERROR] MongoDB Backup Cleanup Failed on $(hostname)" "$ERROR_MSG"
            exit 1
          fi

          # Process each backup
          echo "$BACKUP_LIST" | while read -r line; do
            # Skip empty lines
            [ -z "$line" ] && continue

            FILE_DATE=$(echo "$line" | awk '{print $1" "$2}')
            FILE_PATH=$(echo "$line" | awk '{print $4}')

            # Extract timestamp from filename (ISO 8601 format: mongo-backup-2025-11-27T19:50:03Z.gz.gpg)
            # Filename format: mongo-backup-YYYY-MM-DDTHH:MM:SSZ.gz.gpg
            FILENAME=$(basename "$FILE_PATH")
            ISO_TIMESTAMP=$(echo "$FILENAME" | grep -oP 'mongo-backup-\K[0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}:[0-9]{2}:[0-9]{2}Z' || echo "")

            # Try to parse timestamp from filename first (more reliable), fallback to S3 metadata date
            if [ -n "$ISO_TIMESTAMP" ]; then
              # Parse ISO 8601 timestamp from filename
              if ! FILE_TIMESTAMP=$(date -d "$ISO_TIMESTAMP" +%s 2>/dev/null); then
                echo "Warning: Could not parse ISO timestamp from filename ${FILENAME}, trying S3 date"
                if ! FILE_TIMESTAMP=$(date -d "$FILE_DATE" +%s 2>/dev/null); then
                  echo "Warning: Could not parse date for ${FILE_PATH}, skipping"
                  ERRORS=$((ERRORS + 1))
                  continue
                fi
              fi
            else
              # Fallback to S3 metadata date
              if ! FILE_TIMESTAMP=$(date -d "$FILE_DATE" +%s 2>/dev/null); then
                echo "Warning: Could not parse date for ${FILE_PATH}, skipping"
                ERRORS=$((ERRORS + 1))
                continue
              fi
            fi

            # Delete backups older than 30 days
            if [ "$FILE_TIMESTAMP" -lt "$THIRTY_DAYS_AGO" ]; then
              echo "Deleting backup older than 30 days: ${FILE_PATH}"
              if aws s3 rm "s3://${BACKUP_BUCKET}/${FILE_PATH}" --endpoint-url="${AWS_ENDPOINT_URL}" 2>&1; then
                DELETED_OLD=$((DELETED_OLD + 1))
              else
                echo "ERROR: Failed to delete ${FILE_PATH}"
                ERRORS=$((ERRORS + 1))
              fi

            # For backups 8-30 days old, keep only one per day
            elif [ "$FILE_TIMESTAMP" -lt "$SEVEN_DAYS_AGO" ]; then
              # Extract date from ISO 8601 timestamp in filename (YYYY-MM-DD)
              BACKUP_DATE=$(echo "$ISO_TIMESTAMP" | cut -d'T' -f1)

              if [ -z "$BACKUP_DATE" ]; then
                echo "Warning: Could not extract date from timestamp ${ISO_TIMESTAMP}, keeping file"
                KEPT_RECENT=$((KEPT_RECENT + 1))
                continue
              fi

              # Track if we've already kept one backup for this date
              STATE_FILE="/tmp/mongo-cleanup-${BACKUP_DATE}.keep"

              if [ -f "$STATE_FILE" ]; then
                # Already kept one backup for this date, delete this one
                echo "Deleting non-latest daily backup for ${BACKUP_DATE}: ${FILE_PATH}"
                if aws s3 rm "s3://${BACKUP_BUCKET}/${FILE_PATH}" --endpoint-url="${AWS_ENDPOINT_URL}" 2>&1; then
                  DELETED_DAILY=$((DELETED_DAILY + 1))
                else
                  echo "ERROR: Failed to delete ${FILE_PATH}"
                  ERRORS=$((ERRORS + 1))
                fi
              else
                # First backup for this date, keep it and mark as kept
                echo "Keeping daily backup for ${BACKUP_DATE}: ${FILE_PATH}"
                touch "$STATE_FILE"
                KEPT_RECENT=$((KEPT_RECENT + 1))
              fi
            else
              # Recent backup (< 7 days), keep it
              KEPT_RECENT=$((KEPT_RECENT + 1))
            fi
          done

          # Summary report
          # Clean up state files
          rm -f /tmp/mongo-cleanup-*.keep

          echo ""
          echo "=== MongoDB Backup Cleanup Summary ==="
          echo "Deleted (>30 days): ${DELETED_OLD}"
          echo "Deleted (daily consolidation): ${DELETED_DAILY}"
          echo "Kept (recent <7 days): ${KEPT_RECENT}"
          echo "Errors: ${ERRORS}"
          echo "Completed at: $(date -u +%Y-%m-%dT%H:%M:%SZ)"

          # Send email alert if there were errors
          if [ "$ERRORS" -gt 0 ]; then
            ERROR_MSG="MongoDB backup cleanup completed with ${ERRORS} errors. Check logs: sudo journalctl -u mongodb-backup.service -n 200"
            echo "WARNING: $ERROR_MSG"
            /usr/local/bin/send-rate-limited-email.sh "mongo-cleanup" "[WARNING] MongoDB Backup Cleanup Had Errors on $(hostname)" "$ERROR_MSG"
          fi

          echo "MongoDB backup cleanup completed successfully"

    # Create systemd service for MongoDB backup
    - name: Create MongoDB backup systemd service
      copy:
        dest: /etc/systemd/system/mongodb-backup.service
        content: |
          # WARNING: This file is managed by Ansible
          # Manual changes will be overwritten on next deployment
          [Unit]
          Description=MongoDB Backup to Cloudflare R2
          OnFailure=failure-notification@%n.service

          [Service]
          Type=oneshot
          RemainAfterExit=no
          TimeoutStartSec=600
          TimeoutStopSec=30
          Environment="AWS_ACCESS_KEY_ID={{ lookup('env', 'AWS_ACCESS_KEY_ID') }}"
          Environment="AWS_SECRET_ACCESS_KEY={{ lookup('env', 'AWS_SECRET_ACCESS_KEY') }}"
          Environment="AWS_ENDPOINT_URL={{ lookup('env', 'AWS_ENDPOINT_URL') }}"
          Environment="AWS_DEFAULT_REGION={{ lookup('env', 'AWS_DEFAULT_REGION') or 'auto' }}"
          Environment="MONGO_BACKUP_BUCKET={{ lookup('env', 'MONGO_BACKUP_BUCKET') | default('forwardemail-backups', true) }}"
          Environment="BACKUP_SECRET={{ lookup('env', 'BACKUP_SECRET') }}"
          Environment="MONGO_PORT={{ lookup('env', 'MONGO_PORT') | default('27017', true) }}"
          Environment="POSTFIX_RCPTS={{ lookup('env', 'POSTFIX_RCPTS') | default('security@forwardemail.net', true) }}"
          ExecStart=/usr/local/bin/backup-mongodb.sh
          ExecStartPost=/usr/local/bin/cleanup-mongodb-backups.sh

    # Create systemd timer for MongoDB backup (every 6 hours)
    - name: Create MongoDB backup systemd timer
      copy:
        dest: /etc/systemd/system/mongodb-backup.timer
        content: |
          # WARNING: This file is managed by Ansible
          # Manual changes will be overwritten on next deployment
          [Unit]
          Description=MongoDB Backup Every 6 Hours

          [Timer]
          OnBootSec=15min
          OnUnitActiveSec=6h

          [Install]
          WantedBy=timers.target

    - name: Enable and start MongoDB backup timer
      systemd:
        daemon_reload: true
        name: mongodb-backup.timer
        enabled: true
        state: started

    # ============================================================================
    # POST-DEPLOYMENT INSTRUCTIONS
    # ============================================================================
    - name: Display manual restart instructions
      shell: |
        cat << 'EOF'
        
        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
        â•‘                                                                        â•‘
        â•‘                    âš ï¸  MANUAL RESTART REQUIRED âš ï¸                      â•‘
        â•‘                                                                        â•‘
        â•‘  MongoDB configuration has been updated but the service was NOT       â•‘
        â•‘  automatically restarted to prevent disruption.                       â•‘
        â•‘                                                                        â•‘
        â•‘  To apply the new configuration, you must MANUALLY restart MongoDB:   â•‘
        â•‘                                                                        â•‘
        â•‘    sudo systemctl restart mongod                                      â•‘
        â•‘                                                                        â•‘
        â•‘  âš ï¸  WARNING: This will:                                              â•‘
        â•‘    â€¢ Disconnect all active MongoDB connections                        â•‘
        â•‘    â€¢ Cause brief service interruption                                 â•‘
        â•‘    â€¢ May take time if large datasets need to be flushed               â•‘
        â•‘                                                                        â•‘
        â•‘  ðŸ’¡ RECOMMENDATION:                                                   â•‘
        â•‘    â€¢ Schedule restart during low-traffic period                       â•‘
        â•‘    â€¢ Notify team before restarting                                    â•‘
        â•‘    â€¢ Monitor logs: sudo journalctl -u mongod -f                       â•‘
        â•‘                                                                        â•‘
        â•‘  To verify MongoDB is running after restart:                          â•‘
        â•‘    mongosh --tls --tlsCAFile /etc/mongodb/ssl/ca.pem \               â•‘
        â•‘            --host localhost:27017 --eval "db.adminCommand('ping')"   â•‘
        â•‘                                                                        â•‘
        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
EOF
          
