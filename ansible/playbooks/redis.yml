# Copyright (c) Forward Email LLC
# SPDX-License-Identifier: BUSL-1.1

---
# Valkey version configuration
# To upgrade Valkey, change the version below and update the checksum
# Get checksums from: https://github.com/valkey-io/valkey-hashes
- hosts: redis
  name: Set Valkey Version
  vars:
    valkey_version: "{{ lookup('env', 'VALKEY_VERSION') | default('8.0.6', true) }}"
    # SHA256 checksums for different versions (from official valkey-hashes repo)
    valkey_checksums:
      "8.0.1": "sha256:1e1d6dfbed2f932a87afbc7402be050a73974a9b19a9116897e537a6638e5e1d"
      "8.0.6": "sha256:f8d15c257a3619e0e42e68998e9dc16536009d257662efa4c62ef7d08a71b0dd"
      # Add more versions here as needed
  tasks:
    - name: Set Valkey version facts
      set_fact:
        valkey_version: "{{ valkey_version }}"
        valkey_checksum: "{{ valkey_checksums[valkey_version] | default('') }}"

    - name: Validate Valkey version has checksum
      fail:
        msg: |
          Valkey version {{ valkey_version }} does not have a checksum defined.
          Please add the checksum to the valkey_checksums dictionary in redis.yml.
          Get checksums from: https://github.com/valkey-io/valkey-hashes
      when: valkey_checksum == ''

- name: Import security playbook
  import_playbook: security.yml
- name: Import SSH keys playbook
  import_playbook: ssh-keys.yml
- name: Import UFW allowlist playbook for Redis
  import_playbook: ufw-allowlist.yml
  vars:
    target_hosts: redis
    service_name: Redis
    service_port_var: REDIS_PORT
    service_port_default: "6380"
    service_identifier: redis
    ufw_comment: "Auto-whitelist Redis TLS"

- name: Import high-traffic sysctl tuning for Redis
  import_playbook: sysctl-high-traffic.yml
  vars:
    target_hosts: redis
    service_name: Redis
    sysctl_file_name: redis

- name: Import I/O and filesystem tuning for Redis
  import_playbook: io-filesystem-tuning.yml
  vars:
    target_hosts: redis
    service_name: Redis
    service_identifier: redis
    data_directory: /var/lib/valkey
    read_ahead_kb: 16


- hosts: redis
  name: Certificate Paths
  vars_prompt:
    - name: input_key
      prompt: "Enter path to certificate private key file on local machine (e.g. /path/to/.ssl-key) [REQUIRED]"
      private: false
    - name: input_cert
      prompt: "Enter path to certificate full chain/certificate file on local machine (e.g. /path/to/.ssl-cert) [REQUIRED]"
      private: false
    - name: input_ca
      prompt: "Enter path to certificate CA bundle file on local machine (e.g. /path/to/.ssl-ca) [REQUIRED]"
      private: false
  tasks:
    - name: Fail if key path is empty
      fail:
        msg: "Certificate private key path is required. Please re-run the playbook and provide a valid path."
      when: (input_key is not defined) or (input_key | length == 0)

    - name: Fail if cert path is empty
      fail:
        msg: "Certificate file path is required. Please re-run the playbook and provide a valid path."
      when: (input_cert is not defined) or (input_cert | length == 0)

    - name: Fail if CA path is empty
      fail:
        msg: "CA bundle file path is required. Please re-run the playbook and provide a valid path."
      when: (input_ca is not defined) or (input_ca | length == 0)

    - name: Check if key file exists locally
      local_action: stat path={{ input_key }}
      become: false
      register: local_key_file

    - name: Fail when local key file does not exist
      fail:
        msg: "key file does not exist: {{ input_key }}"
      when: not local_key_file.stat.exists

    - name: Check if cert file exists locally
      local_action: stat path={{ input_cert }}
      become: false
      register: local_cert_file

    - name: Fail when local cert file does not exist
      fail:
        msg: "cert file does not exist: {{ input_cert }}"
      when: not local_cert_file.stat.exists

    - name: Check if CA file exists locally
      local_action: stat path={{ input_ca }}
      become: false
      register: local_ca_file

    - name: Fail when local CA file does not exist
      fail:
        msg: "CA file does not exist: {{ input_ca }}"
      when: not local_ca_file.stat.exists

    - name: Store certificate paths as facts
      set_fact:
        ssl_key_path: "{{ input_key }}"
        ssl_cert_path: "{{ input_cert }}"
        ssl_ca_path: "{{ input_ca }}"

- hosts: redis
  name: Hostname
  become: true
  become_user: root
  tasks:
    - name: Validate required environment variables
      assert:
        that:
          - lookup('env', 'REDIS_HOST') != ''
          - lookup('env', 'REDIS_PASSWORD') != ''
          - lookup('env', 'REDIS_PORT') != ''
          - lookup('env', 'AWS_ACCESS_KEY_ID') != ''
          - lookup('env', 'AWS_SECRET_ACCESS_KEY') != ''
          - lookup('env', 'AWS_ENDPOINT_URL') != ''
          - lookup('env', 'BACKUP_SECRET') != ''
        fail_msg: |
          Required environment variables are missing for Redis playbook.
          Please ensure the following variables are set in your .env file:
          - REDIS_HOST
          - REDIS_PASSWORD
          - REDIS_PORT
          - SSL_CERT_PATH
          - SSL_KEY_PATH
          - SSL_CA_PATH
          - AWS_ACCESS_KEY_ID
          - AWS_SECRET_ACCESS_KEY
          - AWS_ENDPOINT_URL
          - BACKUP_SECRET
        quiet: false

    - name: Set hostname
      hostname:
        name: "{{ lookup('env', 'REDIS_HOST') }}"

- hosts: redis
  name: Redis
  become: true
  become_user: root
  handlers:
    - name: Reload UFW
      ufw:
        state: reloaded
    # NOTE: Valkey restart removed - must be done manually
    # See post-deployment instructions for manual restart procedure
  tasks:
    # Redis-specific sysctl optimizations
    # Note: Common high-traffic settings imported from sysctl-high-traffic.yml
    # This is Redis-specific addition
    - name: Configure Redis-specific sysctl parameters
      ansible.builtin.sysctl:
        name: "{{ item.name }}"
        value: "{{ item.value }}"
        state: present
        reload: true
        sysctl_file: /etc/sysctl.d/99-redis-specific.conf
      loop:
        - { name: 'vm.overcommit_memory', value: '1' }  # Required for Redis background saves

    # Configure swap for Redis stability
    # Redis official docs: "swap file size is equal to amount of memory on your system"
    # https://redis.io/docs/latest/operate/oss_and_stack/management/admin/
    - name: Get total system memory in MB
      shell: free -m | awk '/^Mem:/{print $2}'
      register: total_memory_mb
      changed_when: false

    - name: Check if swap file exists
      stat:
        path: /swapfile
      register: swapfile_stat

    - name: Check current swap size
      shell: swapon --show=SIZE --noheadings --bytes | head -1
      register: current_swap_bytes
      changed_when: false
      failed_when: false

    - name: Calculate required swap size in bytes
      set_fact:
        required_swap_mb: "{{ total_memory_mb.stdout | int }}"
        required_swap_bytes: "{{ (total_memory_mb.stdout | int) * 1024 * 1024 }}"

    - name: Disable existing swap if size doesn't match
      command: swapoff /swapfile
      when:
        - swapfile_stat.stat.exists
        - current_swap_bytes.stdout != ""
        - current_swap_bytes.stdout | int != required_swap_bytes | int
      ignore_errors: true

    - name: Remove old swap file if size doesn't match
      file:
        path: /swapfile
        state: absent
      when:
        - swapfile_stat.stat.exists
        - current_swap_bytes.stdout != ""
        - current_swap_bytes.stdout | int != required_swap_bytes | int

    - name: Create swap file with size equal to RAM
      command: fallocate -l {{ required_swap_mb }}M /swapfile
      args:
        creates: /swapfile

    - name: Set swap file permissions
      file:
        path: /swapfile
        mode: '0600'
        owner: root
        group: root

    - name: Format swap file
      command: mkswap /swapfile
      when: not swapfile_stat.stat.exists or (current_swap_bytes.stdout | int != required_swap_bytes | int)
      register: mkswap_result
      changed_when: "'Setting up swapspace' in mkswap_result.stdout"

    - name: Enable swap file
      command: swapon /swapfile
      when: mkswap_result is changed

    - name: Add swap to /etc/fstab
      lineinfile:
        path: /etc/fstab
        line: '/swapfile none swap sw 0 0'
        state: present
        regexp: '^/swapfile'

    - name: Set swappiness for Redis (minimize swapping but allow it for safety)
      ansible.builtin.sysctl:
        name: vm.swappiness
        value: '1'
        state: present
        reload: true
        sysctl_file: /etc/sysctl.d/99-redis.conf

    # Install Valkey 8.x from source
    - name: Install build dependencies
      apt:
        name:
          - build-essential
          - tcl
          - pkg-config
          - libssl-dev
        state: present
        update_cache: yes

    - name: Create valkey user
      user:
        name: valkey
        system: yes
        shell: /bin/false
        home: /var/lib/valkey
        create_home: yes

    - name: Create valkey group
      group:
        name: valkey
        system: yes

    - name: Download Valkey {{ hostvars['localhost']['valkey_version'] | default('8.0.6') }} source
      get_url:
        url: https://github.com/valkey-io/valkey/archive/refs/tags/{{ hostvars['localhost']['valkey_version'] | default('8.0.6') }}.tar.gz
        dest: /var/tmp/valkey-{{ hostvars['localhost']['valkey_version'] | default('8.0.6') }}.tar.gz
        checksum: "{{ hostvars['localhost']['valkey_checksum'] | default('sha256:f8d15c257a3619e0e42e68998e9dc16536009d257662efa4c62ef7d08a71b0dd') }}"

    - name: Extract Valkey source
      unarchive:
        src: /var/tmp/valkey-{{ hostvars['localhost']['valkey_version'] | default('8.0.6') }}.tar.gz
        dest: /var/tmp/
        remote_src: yes
        creates: /var/tmp/valkey-{{ hostvars['localhost']['valkey_version'] | default('8.0.6') }}

    - name: Compile Valkey
      shell: |
        cd /var/tmp/valkey-{{ hostvars['localhost']['valkey_version'] | default('8.0.6') }}
        make BUILD_TLS=yes
      args:
        creates: /var/tmp/valkey-{{ hostvars['localhost']['valkey_version'] | default('8.0.6') }}/src/valkey-server

    - name: Install Valkey binaries
      shell: |
        cd /var/tmp/valkey-{{ hostvars['localhost']['valkey_version'] | default('8.0.6') }}
        make install PREFIX=/usr
      args:
        creates: /usr/bin/valkey-server

    - name: Create Valkey directories
      file:
        path: "{{ item }}"
        state: directory
        owner: valkey
        group: valkey
        mode: '0755'
      loop:
        - /etc/valkey
        - /var/lib/valkey
        - /var/log/valkey

    - name: Create Valkey systemd service
      copy:
        dest: /etc/systemd/system/valkey-server.service
        content: |
          # WARNING: This file is managed by Ansible
          # Manual changes will be overwritten on next deployment
          [Unit]
          Description=Valkey In-Memory Data Store
          After=network.target
          OnFailure=failure-notification@%n.service

          [Service]
          Type=simple
          User=valkey
          Group=valkey
          ExecStart=/usr/bin/valkey-server /etc/valkey/valkey.conf
          ExecStop=/bin/kill -s TERM $MAINPID
          PIDFile=/var/run/valkey/valkey-server.pid
          TimeoutStartSec=90
          TimeoutStopSec=300
          Restart=always
          RestartSec=5s
          RuntimeDirectory=valkey
          RuntimeDirectoryMode=0755

          [Install]
          WantedBy=multi-user.target

    - name: Reload systemd
      systemd:
        daemon_reload: yes

    # Create SSL directory for Valkey certificates
    - name: Create /etc/valkey/ssl directory
      file:
        path: /etc/valkey/ssl
        state: directory
        owner: valkey
        group: valkey
        mode: "0750"

    # Copy SSL certificates directly from local machine to Valkey directory
    - name: Copy SSL certificate to Valkey directory
      copy:
        src: "{{ hostvars[inventory_hostname]['ssl_cert_path'] }}"
        dest: /etc/valkey/ssl/valkey.crt
        owner: valkey
        group: valkey
        mode: "0400"

    - name: Copy SSL private key to Valkey directory
      copy:
        src: "{{ hostvars[inventory_hostname]['ssl_key_path'] }}"
        dest: /etc/valkey/ssl/valkey.key
        owner: valkey
        group: valkey
        mode: "0400"

    - name: Copy CA certificate to Valkey directory
      copy:
        src: "{{ hostvars[inventory_hostname]['ssl_ca_path'] }}"
        dest: /etc/valkey/ssl/ca.pem
        owner: valkey
        group: valkey
        mode: "0400"

    # Calculate dynamic resource limits
    - name: Get total system memory in MB
      shell: free -m | awk '/^Mem:/{print $2}'
      register: total_memory_mb
      changed_when: false

    - name: Calculate maxmemory (75% of total RAM)
      set_fact:
        valkey_maxmemory_mb: "{{ (total_memory_mb.stdout | int * 0.75) | int }}"

    - name: Get number of CPU cores
      shell: nproc
      register: cpu_cores
      changed_when: false

    - name: Set io-threads (75% of CPU cores for optimal performance, max 8)
      set_fact:
        valkey_io_threads: "{{ [(cpu_cores.stdout | int * 0.75) | int, 8] | min }}"

    - name: Create base Valkey configuration file
      copy:
        dest: /etc/valkey/valkey.conf
        owner: valkey
        group: valkey
        mode: '0640'
        force: yes
        content: |
          # WARNING: This file is managed by Ansible
          # Manual changes will be overwritten on next deployment
          # Valkey configuration file
          # Generated by Ansible - base configuration with security hardening
          # Additional custom settings are managed in the ANSIBLE MANAGED BLOCK below

          ################################## GENERAL #####################################

          # By default the server does not run as a daemon.
          daemonize no

          # Supervised by systemd
          supervised systemd

          # PID file location
          pidfile /var/run/valkey/valkey-server.pid

          # Logging
          loglevel notice
          logfile /var/log/valkey/valkey-server.log

          # Set the number of databases
          databases 16

          ################################## SNAPSHOTTING ################################

          # Save the DB to disk (handled by ANSIBLE MANAGED BLOCK)
          # Default: save 3600 1 300 100 60 10000

          # Compression of string objects using LZF when dump .rdb databases
          rdbcompression yes

          # RDB file checksum
          rdbchecksum yes

          # The filename where to dump the DB
          dbfilename dump.rdb

          # The working directory
          dir /var/lib/valkey

          ################################# REPLICATION ##################################

          # Disable replication by default
          # replicaof <masterip> <masterport>

          ################################## SECURITY ####################################

          # Require clients to authenticate (password set in ANSIBLE MANAGED BLOCK)
          # requirepass <password>

          # Disable dangerous commands
          rename-command FLUSHDB ""
          rename-command FLUSHALL ""
          # rename-command KEYS ""
          rename-command CONFIG ""
          rename-command SHUTDOWN ""
          rename-command BGREWRITEAOF ""
          # rename-command BGSAVE ""  # Re-enabled for backup script
          rename-command SAVE ""
          rename-command DEBUG ""

          ################################### CLIENTS ####################################

          # Max number of connected clients (set in ANSIBLE MANAGED BLOCK)
          # maxclients 10000

          ############################## MEMORY MANAGEMENT ###############################

          # Maximum memory limit (75% of total system RAM)
          maxmemory {{ valkey_maxmemory_mb }}mb

          # Memory eviction policy
          maxmemory-policy volatile-ttl

          # LRU and minimal TTL algorithms are not precise but approximated
          maxmemory-samples 5

          ############################# LAZY FREEING ####################################

          # Lazy freeing to avoid blocking on DEL
          lazyfree-lazy-eviction yes
          lazyfree-lazy-expire yes
          lazyfree-lazy-server-del yes
          replica-lazy-flush yes

          ############################## APPEND ONLY MODE ###############################

          # AOF disabled - using RDB-only for session/cache store
          # High write frequency (1000s/sec) makes AOF overhead too high
          # Acceptable data loss window: last RDB snapshot to crash
          appendonly no

          # AOF settings (unused but kept for reference)
          # auto-aof-rewrite-percentage 100
          # auto-aof-rewrite-min-size 64mb
          # aof-load-truncated yes
          # aof-use-rdb-preamble yes

          ################################ THREADED I/O #################################

          # Enable threaded I/O for better performance
          # Number of I/O threads (dynamically set to number of CPU cores, max 8)
          io-threads {{ valkey_io_threads }}
          io-threads-do-reads yes

          ################################## SLOW LOG ###################################

          # Log queries slower than this many microseconds
          slowlog-log-slower-than 10000
          slowlog-max-len 128

          ################################ LATENCY MONITOR ##############################

          # Latency monitoring
          latency-monitor-threshold 100

          ############################# EVENT NOTIFICATION ###############################

          # Keyspace notifications (disabled by default for performance)
          notify-keyspace-events ""

          ############################### ADVANCED CONFIG ################################

          # Hashes are encoded using a memory efficient data structure when small
          hash-max-listpack-entries 512
          hash-max-listpack-value 64

          # Lists are encoded in a special way to save space
          list-max-listpack-size -2
          list-compress-depth 0

          # Sets have a special encoding when composed of just strings
          set-max-intset-entries 512
          set-max-listpack-entries 128
          set-max-listpack-value 64

          # Sorted sets are encoded to save space
          zset-max-listpack-entries 128
          zset-max-listpack-value 64

          # HyperLogLog sparse representation bytes limit
          hll-sparse-max-bytes 3000

          # Streams macro node max size / items
          stream-node-max-bytes 4096
          stream-node-max-entries 100

          # Active rehashing
          activerehashing yes

          # Client output buffer limits
          client-output-buffer-limit normal 0 0 0
          client-output-buffer-limit replica 256mb 64mb 60
          client-output-buffer-limit pubsub 32mb 8mb 60

          # Frequency of rehashing the main dict
          # Increased from 10 to 50 for faster background tasks (key expiration, client timeout)
          # Trade-off: +1-2% CPU for significantly faster TTL processing
          hz 50

          # Enable active defragmentation
          activedefrag yes
          active-defrag-ignore-bytes 100mb
          active-defrag-threshold-lower 10
          active-defrag-threshold-upper 100
          active-defrag-cycle-min 5
          active-defrag-cycle-max 75

    - name: Configure Valkey
      blockinfile:
        path: /etc/valkey/valkey.conf
        marker: "# {mark} ANSIBLE MANAGED BLOCK - VALKEY CONFIG"
        create: yes
        block: |
          # Bind to all interfaces
          bind 0.0.0.0 ::

          # Authentication
          requirepass {{ lookup('env', 'REDIS_PASSWORD') }}

          # TLS/SSL Configuration
          tls-port {{ lookup('env', 'REDIS_PORT') }}
          port 0
          tls-cert-file /etc/valkey/ssl/valkey.crt
          tls-key-file /etc/valkey/ssl/valkey.key
          tls-ca-cert-file /etc/valkey/ssl/ca.pem
          tls-auth-clients no

          # Persistence - RDB only for session/cache store
          # AOF disabled due to high write frequency (1000s writes/sec)
          appendonly no

          # RDB snapshot - single rule for maximum throughput
          # 30-minute interval provides crash recovery while minimizing BGSAVE overhead
          # Backup script runs at :15 past the hour (offset to avoid conflicts)
          # Every 30 minutes if any changes (max 30-min data loss)
          save 1800 1

          # Performance Optimizations
          tcp-backlog 65536
          tcp-keepalive 300
          timeout 0
          maxclients 65536

    - name: Ensure Valkey is enabled and started
      service:
        name: valkey-server
        state: started
        enabled: yes

    # UFW Configuration - Using reusable playbook
    # See ufw-allowlist.yml for implementation details

    # Install AWS CLI and GPG dependencies
    - name: Install AWS CLI and GPG dependencies
      apt:
        name:
          - python3-pip
          - unzip
          - gnupg
        update_cache: true

    - name: Install AWS CLI
      shell: |
        if ! command -v aws &> /dev/null; then
          curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "/tmp/awscliv2.zip"
          unzip -q /tmp/awscliv2.zip -d /tmp
          /tmp/aws/install
          rm -rf /tmp/aws /tmp/awscliv2.zip
        fi
      args:
        creates: /usr/local/bin/aws

    # Create Redis backup script
    - name: Create Redis backup script
      copy:
        dest: /usr/local/bin/backup-redis.sh
        mode: "0755"
        content: |
          #!/bin/bash
          set -e

          # Redirect stderr to stdout so all output goes to journal
          exec 2>&1

          # Redis Backup Script - RDB Only
          # AOF is disabled for this session/cache store due to high write frequency
          # This script creates RDB snapshots and uploads to R2

          # Environment variables (set via systemd service or environment)
          AWS_ACCESS_KEY_ID="${AWS_ACCESS_KEY_ID}"
          AWS_SECRET_ACCESS_KEY="${AWS_SECRET_ACCESS_KEY}"
          AWS_ENDPOINT_URL="${AWS_ENDPOINT_URL}"
          BACKUP_BUCKET="${REDIS_BACKUP_BUCKET:-forwardemail-backups}"
          BACKUP_SECRET="${BACKUP_SECRET}"
          REDIS_HOST="${REDIS_HOST}"
          REDIS_PORT="${REDIS_PORT}"
          REDIS_PASSWORD="${REDIS_PASSWORD}"
          REDIS_DATA_DIR="${REDIS_DATA_DIR:-/var/lib/valkey}"

          # Check if BACKUP_SECRET is set
          if [ -z "$BACKUP_SECRET" ]; then
            echo "Error: BACKUP_SECRET environment variable is not set"
            exit 1
          fi

          # Timestamp and path (flat structure - no folder nesting)
          BACKUP_NAME="redis-backup-$(date -u +%Y-%m-%dT%H:%M:%SZ).rdb.gpg"
          S3_PATH="s3://${BACKUP_BUCKET}/redis/${BACKUP_NAME}"

          # Build valkey-cli connection command
          VALKEY_CMD="valkey-cli -h ${REDIS_HOST} -p ${REDIS_PORT}"
          if [ -n "${REDIS_PASSWORD}" ]; then
            VALKEY_CMD="${VALKEY_CMD} -a ${REDIS_PASSWORD}"
          fi
          # Add TLS flags if using TLS port
          if [ "${REDIS_PORT}" != "6379" ]; then
            VALKEY_CMD="${VALKEY_CMD} --tls --insecure"
          fi

          # Get LASTSAVE timestamp BEFORE triggering BGSAVE
          INITIAL_SAVE=$(${VALKEY_CMD} LASTSAVE)
          echo "Current LASTSAVE: $INITIAL_SAVE"

          # Trigger Valkey BGSAVE
          echo "Triggering BGSAVE on ${REDIS_HOST}:${REDIS_PORT}..."
          BGSAVE_RESULT=$(${VALKEY_CMD} BGSAVE 2>&1)
          echo "BGSAVE response: $BGSAVE_RESULT"

          # Check if BGSAVE was accepted or already in progress
          if echo "$BGSAVE_RESULT" | grep -qE "Background saving started|Background save already in progress"; then
            echo "BGSAVE initiated successfully"
          else
            ERROR_MSG="Failed to trigger BGSAVE on ${REDIS_HOST}:${REDIS_PORT}: $BGSAVE_RESULT"
            /usr/local/bin/send-rate-limited-email.sh "redis-backup" "[ERROR] Redis Backup Failed on $(hostname)" "$ERROR_MSG"
            exit 1
          fi

          # Wait for BGSAVE to complete (max 5 minutes)
          echo "Waiting for BGSAVE to complete..."
          TIMEOUT=300  # 5 minutes
          ELAPSED=0
          while [ $ELAPSED -lt $TIMEOUT ]; do
            sleep 2
            ELAPSED=$((ELAPSED + 2))
            CURRENT_SAVE=$(${VALKEY_CMD} LASTSAVE)

            # Check if LASTSAVE has changed (BGSAVE completed)
            if [ "$CURRENT_SAVE" != "$INITIAL_SAVE" ]; then
              echo "BGSAVE completed. New LASTSAVE: $CURRENT_SAVE"
              break
            fi

            # Show progress every 10 seconds
            if [ $((ELAPSED % 10)) -eq 0 ]; then
              echo "Still waiting... (${ELAPSED}s elapsed)"
            fi
          done

          # Check if we timed out
          if [ $ELAPSED -ge $TIMEOUT ]; then
            ERROR_MSG="BGSAVE timed out after ${TIMEOUT} seconds"
            /usr/local/bin/send-rate-limited-email.sh "redis-backup" "[ERROR] Redis Backup Failed on $(hostname)" "$ERROR_MSG"
            exit 1
          fi

          # Give it a moment to finish writing
          sleep 2

          # Use local dump.rdb file
          RDB_FILE="${REDIS_DATA_DIR}/dump.rdb"
          if [ ! -f "${RDB_FILE}" ]; then
            ERROR_MSG="RDB file not found at ${RDB_FILE}"
            /usr/local/bin/send-rate-limited-email.sh "redis-backup" "[ERROR] Redis Backup Failed on $(hostname)" "$ERROR_MSG"
            exit 1
          fi

          # Check if bucket exists, create if not
          echo "Checking if S3 bucket exists: ${BACKUP_BUCKET}"
          if ! aws s3 ls "s3://${BACKUP_BUCKET}" --endpoint-url="${AWS_ENDPOINT_URL}" >/dev/null 2>&1; then
            echo "Bucket does not exist, creating: ${BACKUP_BUCKET}"
            if ! aws s3 mb "s3://${BACKUP_BUCKET}" --endpoint-url="${AWS_ENDPOINT_URL}" 2>&1; then
              ERROR_MSG="Failed to create S3 bucket ${BACKUP_BUCKET} at $(date)"
              echo "ERROR: $ERROR_MSG"
              /usr/local/bin/send-rate-limited-email.sh "redis-backup" "[ERROR] Redis Backup Failed on $(hostname)" "$ERROR_MSG"
              exit 1
            fi
            echo "Bucket created successfully: ${BACKUP_BUCKET}"

            # Harden bucket security
            echo "Applying security hardening to bucket..."

            # Block public access (R2 specific)
            if aws s3api put-public-access-block \
              --bucket "${BACKUP_BUCKET}" \
              --public-access-block-configuration "BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true" \
              --endpoint-url="${AWS_ENDPOINT_URL}" 2>/dev/null; then
              echo "âœ“ Public access blocked"
            else
              echo "âš  Could not set public access block (may not be supported by R2)"
            fi

            # Set private ACL
            if aws s3api put-bucket-acl \
              --bucket "${BACKUP_BUCKET}" \
              --acl private \
              --endpoint-url="${AWS_ENDPOINT_URL}" 2>/dev/null; then
              echo "âœ“ Bucket ACL set to private"
            else
              echo "âš  Could not set bucket ACL (may not be supported by R2)"
            fi

            echo "Bucket security hardening completed"
          else
            echo "Bucket exists: ${BACKUP_BUCKET}"
          fi

          # Encrypt and upload RDB file to S3
          echo "Encrypting and uploading backup to ${S3_PATH}..."
          echo "RDB file size: $(du -h ${RDB_FILE} | cut -f1)"

          # Run encryption and upload with error capture
          if ! gpg --symmetric --cipher-algo AES256 --batch --yes --passphrase "$BACKUP_SECRET" < "${RDB_FILE}" | \
            aws s3 cp - "${S3_PATH}" --endpoint-url="${AWS_ENDPOINT_URL}" 2>&1; then
            ERROR_MSG="Failed to encrypt/upload backup at $(date). Check AWS credentials and network connectivity."
            echo "ERROR: $ERROR_MSG"
            /usr/local/bin/send-rate-limited-email.sh "redis-backup" "[ERROR] Redis Backup Failed on $(hostname)" "$ERROR_MSG"
            exit 1
          fi

          echo "Redis encrypted backup completed: ${S3_PATH}"

    # Create Redis backup cleanup script
    - name: Create Redis backup cleanup script
      copy:
        dest: /usr/local/bin/cleanup-redis-backups.sh
        mode: "0755"
        content: |
          #!/bin/bash
          set -e
          exec 2>&1  # Redirect stderr to stdout for journal logging

          echo "Starting Redis backup cleanup at $(date -u +%Y-%m-%dT%H:%M:%SZ)"

          # Environment variables
          AWS_ACCESS_KEY_ID="${AWS_ACCESS_KEY_ID}"
          AWS_SECRET_ACCESS_KEY="${AWS_SECRET_ACCESS_KEY}"
          AWS_ENDPOINT_URL="${AWS_ENDPOINT_URL}"
          AWS_DEFAULT_REGION="${AWS_DEFAULT_REGION:-auto}"
          BACKUP_BUCKET="${REDIS_BACKUP_BUCKET:-forwardemail-backups}"

          # Counters for reporting
          DELETED_OLD=0
          DELETED_DAILY=0
          KEPT_RECENT=0
          ERRORS=0

          # Calculate dates
          THIRTY_DAYS_AGO=$(date -d "30 days ago" +%s)
          SEVEN_DAYS_AGO=$(date -d "7 days ago" +%s)

          echo "Retention policy: Keep all backups < 7 days, one per day for 8-30 days, delete > 30 days"
          echo "Checking bucket: s3://${BACKUP_BUCKET}/redis/"

          # List all backups and process
          if ! BACKUP_LIST=$(aws s3 ls s3://${BACKUP_BUCKET}/redis/ --recursive --endpoint-url="${AWS_ENDPOINT_URL}" 2>&1); then
            ERROR_MSG="Failed to list backups in s3://${BACKUP_BUCKET}/redis/ at $(date -u +%Y-%m-%dT%H:%M:%SZ). Error: ${BACKUP_LIST}"
            echo "ERROR: $ERROR_MSG"
            /usr/local/bin/send-rate-limited-email.sh "redis-cleanup" "[ERROR] Redis Backup Cleanup Failed on $(hostname)" "$ERROR_MSG"
            exit 1
          fi

          # Process each backup
          echo "$BACKUP_LIST" | while read -r line; do
            # Skip empty lines
            [ -z "$line" ] && continue

            FILE_DATE=$(echo "$line" | awk '{print $1" "$2}')
            FILE_PATH=$(echo "$line" | awk '{print $4}')

            # Extract timestamp from filename (ISO 8601 format: redis-backup-2025-11-27T19:50:03Z.rdb.gpg)
            # Filename format: redis-backup-YYYY-MM-DDTHH:MM:SSZ.rdb.gpg
            FILENAME=$(basename "$FILE_PATH")
            ISO_TIMESTAMP=$(echo "$FILENAME" | grep -oP 'redis-backup-\K[0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}:[0-9]{2}:[0-9]{2}Z' || echo "")

            # Try to parse timestamp from filename first (more reliable), fallback to S3 metadata date
            if [ -n "$ISO_TIMESTAMP" ]; then
              # Parse ISO 8601 timestamp from filename
              if ! FILE_TIMESTAMP=$(date -d "$ISO_TIMESTAMP" +%s 2>/dev/null); then
                echo "Warning: Could not parse ISO timestamp from filename ${FILENAME}, trying S3 date"
                if ! FILE_TIMESTAMP=$(date -d "$FILE_DATE" +%s 2>/dev/null); then
                  echo "Warning: Could not parse date for ${FILE_PATH}, skipping"
                  ERRORS=$((ERRORS + 1))
                  continue
                fi
              fi
            else
              # Fallback to S3 metadata date
              if ! FILE_TIMESTAMP=$(date -d "$FILE_DATE" +%s 2>/dev/null); then
                echo "Warning: Could not parse date for ${FILE_PATH}, skipping"
                ERRORS=$((ERRORS + 1))
                continue
              fi
            fi

            # Delete backups older than 30 days
            if [ "$FILE_TIMESTAMP" -lt "$THIRTY_DAYS_AGO" ]; then
              echo "Deleting backup older than 30 days: ${FILE_PATH}"
              if aws s3 rm "s3://${BACKUP_BUCKET}/${FILE_PATH}" --endpoint-url="${AWS_ENDPOINT_URL}" 2>&1; then
                DELETED_OLD=$((DELETED_OLD + 1))
              else
                echo "ERROR: Failed to delete ${FILE_PATH}"
                ERRORS=$((ERRORS + 1))
              fi

            # For backups 8-30 days old, keep only one per day
            elif [ "$FILE_TIMESTAMP" -lt "$SEVEN_DAYS_AGO" ]; then
              # Extract date from ISO 8601 timestamp in filename (YYYY-MM-DD)
              BACKUP_DATE=$(echo "$ISO_TIMESTAMP" | cut -d'T' -f1)

              if [ -z "$BACKUP_DATE" ]; then
                echo "Warning: Could not extract date from timestamp ${ISO_TIMESTAMP}, keeping file"
                KEPT_RECENT=$((KEPT_RECENT + 1))
                continue
              fi

              # Track if we've already kept one backup for this date
              STATE_FILE="/tmp/redis-cleanup-${BACKUP_DATE}.keep"

              if [ -f "$STATE_FILE" ]; then
                # Already kept one backup for this date, delete this one
                echo "Deleting non-latest daily backup for ${BACKUP_DATE}: ${FILE_PATH}"
                if aws s3 rm "s3://${BACKUP_BUCKET}/${FILE_PATH}" --endpoint-url="${AWS_ENDPOINT_URL}" 2>&1; then
                  DELETED_DAILY=$((DELETED_DAILY + 1))
                else
                  echo "ERROR: Failed to delete ${FILE_PATH}"
                  ERRORS=$((ERRORS + 1))
                fi
              else
                # First backup for this date, keep it and mark as kept
                echo "Keeping daily backup for ${BACKUP_DATE}: ${FILE_PATH}"
                touch "$STATE_FILE"
                KEPT_RECENT=$((KEPT_RECENT + 1))
              fi
            else
              # Recent backup (< 7 days), keep it
              KEPT_RECENT=$((KEPT_RECENT + 1))
            fi
          done

          # Summary report
          # Clean up state files
          rm -f /tmp/redis-cleanup-*.keep

          echo ""
          echo "=== Redis Backup Cleanup Summary ==="
          echo "Deleted (>30 days): ${DELETED_OLD}"
          echo "Deleted (daily consolidation): ${DELETED_DAILY}"
          echo "Kept (recent <7 days): ${KEPT_RECENT}"
          echo "Errors: ${ERRORS}"
          echo "Completed at: $(date -u +%Y-%m-%dT%H:%M:%SZ)"

          # Send email alert if there were errors
          if [ "$ERRORS" -gt 0 ]; then
            ERROR_MSG="Redis backup cleanup completed with ${ERRORS} errors. Check logs: sudo journalctl -u redis-backup.service -n 200"
            echo "WARNING: $ERROR_MSG"
            /usr/local/bin/send-rate-limited-email.sh "redis-cleanup" "[WARNING] Redis Backup Cleanup Had Errors on $(hostname)" "$ERROR_MSG"
          fi

          echo "Redis backup cleanup completed successfully"

    # Create systemd service for Redis backup
    - name: Create Redis backup systemd service
      copy:
        dest: /etc/systemd/system/redis-backup.service
        content: |
          [Unit]
          Description=Redis Backup to Cloudflare R2
          OnFailure=failure-notification@%n.service

          [Service]
          Type=oneshot
          RemainAfterExit=no
          TimeoutStartSec=600
          TimeoutStopSec=30
          Environment="AWS_ACCESS_KEY_ID={{ lookup('env', 'AWS_ACCESS_KEY_ID') }}"
          Environment="AWS_SECRET_ACCESS_KEY={{ lookup('env', 'AWS_SECRET_ACCESS_KEY') }}"
          Environment="AWS_ENDPOINT_URL={{ lookup('env', 'AWS_ENDPOINT_URL') }}"
          Environment="AWS_DEFAULT_REGION={{ lookup('env', 'AWS_DEFAULT_REGION') or 'auto' }}"
          Environment="REDIS_BACKUP_BUCKET={{ lookup('env', 'REDIS_BACKUP_BUCKET') | default('forwardemail-backups', true) }}"
          Environment="BACKUP_SECRET={{ lookup('env', 'BACKUP_SECRET') }}"
          Environment="REDIS_HOST={{ lookup('env', 'REDIS_HOST') }}"
          Environment="REDIS_PORT={{ lookup('env', 'REDIS_PORT') | default(lookup('env', 'REDIS_PORT'), true) }}"
          Environment="REDIS_PORT={{ lookup('env', 'REDIS_PORT') }}"
          Environment="REDIS_PASSWORD={{ lookup('env', 'REDIS_PASSWORD') }}"
          Environment="REDIS_DATA_DIR={{ lookup('env', 'REDIS_DATA_DIR') or '/var/lib/valkey' }}"
          Environment="MSMTP_RCPTS={{ lookup('env', 'MSMTP_RCPTS') | default('security@forwardemail.net', true) }}"
          ExecStart=/usr/local/bin/backup-redis.sh
          ExecStartPost=/usr/local/bin/cleanup-redis-backups.sh

    # Create systemd timer for Redis backup (every 6 hours)
    # Runs at :15 minutes past the hour to avoid conflict with automatic RDB saves
    - name: Create Redis backup systemd timer
      copy:
        dest: /etc/systemd/system/redis-backup.timer
        content: |
          # WARNING: This file is managed by Ansible
          # Manual changes will be overwritten on next deployment
          [Unit]
          Description=Redis Backup Every 6 Hours (at :15 past the hour)

          [Timer]
          # Run at 00:15, 06:15, 12:15, 18:15 (offset from automatic saves)
          OnCalendar=00/6:15:00
          # Fallback: if system was off during scheduled time, run 15 min after boot
          OnBootSec=15min

          [Install]
          WantedBy=timers.target

    - name: Enable and start Redis backup timer
      systemd:
        daemon_reload: true
        name: redis-backup.timer
        enabled: true
        state: started

    # Create Redis command monitoring script
    - name: Create Redis command monitoring script
      copy:
        dest: /usr/local/bin/monitor-redis-commands.sh
        mode: "0755"
        content: |
          #!/bin/bash
          set -e

          # Environment variables
          REDIS_HOST="${REDIS_HOST}"
          REDIS_PORT="${REDIS_PORT}"
          REDIS_PASSWORD="${REDIS_PASSWORD}"
          STATE_DIR="/var/lib/redis-monitoring"
          BGSAVE_STATE_FILE="${STATE_DIR}/bgsave-count.txt"
          KEYS_STATE_FILE="${STATE_DIR}/keys-count.txt"
          BGSAVE_THRESHOLD=2  # Alert if more than 2 BGSAVE calls per hour (backup runs every 6h)
          KEYS_THRESHOLD=5    # Alert if more than 5 KEYS calls per hour (KEYS should rarely be used)

          # Build valkey-cli connection command
          VALKEY_CMD="valkey-cli -h ${REDIS_HOST} -p ${REDIS_PORT}"
          if [ -n "${REDIS_PASSWORD}" ]; then
            VALKEY_CMD="${VALKEY_CMD} -a ${REDIS_PASSWORD}"
          fi
          # Add TLS flags if using TLS port
          if [ "${REDIS_PORT}" != "6379" ]; then
            VALKEY_CMD="${VALKEY_CMD} --tls --insecure"
          fi

          # Create state directory if it doesn't exist
          mkdir -p "$STATE_DIR"

          # Get command stats from Redis
          COMMANDSTATS=$(${VALKEY_CMD} INFO commandstats)
          CONNECTED_CLIENTS=$(${VALKEY_CMD} INFO clients | grep "connected_clients:" | cut -d: -f2 | tr -d '\r')

          # Track BGSAVE usage
          BGSAVE_CURRENT=$(echo "$COMMANDSTATS" | grep "cmdstat_bgsave:" | grep -oP 'calls=\K[0-9]+' || echo "0")
          if [ -f "$BGSAVE_STATE_FILE" ]; then
            BGSAVE_PREVIOUS=$(cat "$BGSAVE_STATE_FILE")
          else
            BGSAVE_PREVIOUS=0
          fi
          BGSAVE_DELTA=$((BGSAVE_CURRENT - BGSAVE_PREVIOUS))

          # Track KEYS usage
          KEYS_CURRENT=$(echo "$COMMANDSTATS" | grep "cmdstat_keys:" | grep -oP 'calls=\K[0-9]+' || echo "0")
          if [ -f "$KEYS_STATE_FILE" ]; then
            KEYS_PREVIOUS=$(cat "$KEYS_STATE_FILE")
          else
            KEYS_PREVIOUS=0
          fi
          KEYS_DELTA=$((KEYS_CURRENT - KEYS_PREVIOUS))

          # Check for alerts
          ALERT_TRIGGERED=0
          ALERT_MSG=""

          # Check BGSAVE threshold
          if [ $BGSAVE_DELTA -gt $BGSAVE_THRESHOLD ]; then
            BGSAVE_STATS=$(echo "$COMMANDSTATS" | grep "cmdstat_bgsave:")
            ALERT_MSG="${ALERT_MSG}âš ï¸  BGSAVE ALERT\n\nBGSAVE calls in last hour: ${BGSAVE_DELTA}\nAlert threshold: ${BGSAVE_THRESHOLD}\nTotal BGSAVE calls: ${BGSAVE_CURRENT}\n\nBGSAVE Stats:\n${BGSAVE_STATS}\n\nThis may indicate:\n- Unauthorized access attempting to dump data\n- Misconfigured backup scripts\n- Manual BGSAVE commands being run\n\n"
            ALERT_TRIGGERED=1
            logger -t redis-command-monitor "SECURITY ALERT: Unusual BGSAVE activity - ${BGSAVE_DELTA} calls in last hour"
          fi

          # Check KEYS threshold
          if [ $KEYS_DELTA -gt $KEYS_THRESHOLD ]; then
            KEYS_STATS=$(echo "$COMMANDSTATS" | grep "cmdstat_keys:")
            ALERT_MSG="${ALERT_MSG}âš ï¸  KEYS ALERT\n\nKEYS calls in last hour: ${KEYS_DELTA}\nAlert threshold: ${KEYS_THRESHOLD}\nTotal KEYS calls: ${KEYS_CURRENT}\n\nKEYS Stats:\n${KEYS_STATS}\n\nThis may indicate:\n- Application using KEYS instead of SCAN (performance issue)\n- Unauthorized access enumerating keys\n- Debugging/monitoring scripts running\n- Potential DoS attack (KEYS blocks Redis)\n\n"
            ALERT_TRIGGERED=1
            logger -t redis-command-monitor "SECURITY ALERT: Unusual KEYS activity - ${KEYS_DELTA} calls in last hour"
          fi

          # Send combined alert if triggered
          if [ $ALERT_TRIGGERED -eq 1 ]; then
            FULL_ALERT="Suspicious Redis command activity detected on $(hostname)\n\n${ALERT_MSG}\nConnected Clients: ${CONNECTED_CLIENTS}\n\nPlease investigate immediately.\n\nTo check recent connections:\nvalkey-cli -h ${REDIS_HOST} -p ${REDIS_PORT} CLIENT LIST\n\nTo check all command stats:\nvalkey-cli -h ${REDIS_HOST} -p ${REDIS_PORT} INFO commandstats\n\nTo check slow log:\nvalkey-cli -h ${REDIS_HOST} -p ${REDIS_PORT} SLOWLOG GET 10\n\nTo check server logs:\njournalctl -u valkey-server -n 100\n"
            /usr/local/bin/send-rate-limited-email.sh "redis-command-monitor" "[SECURITY] Suspicious Redis Command Activity on $(hostname)" "$FULL_ALERT"
          fi

          # Save current counts for next check
          echo "$BGSAVE_CURRENT" > "$BGSAVE_STATE_FILE"
          echo "$KEYS_CURRENT" > "$KEYS_STATE_FILE"

    # Create systemd service for Redis command monitoring
    - name: Create Redis command monitoring systemd service
      copy:
        dest: /etc/systemd/system/redis-command-monitor.service
        content: |
          # WARNING: This file is managed by Ansible
          # Manual changes will be overwritten on next deployment
          [Unit]
          Description=Monitor Redis Command Usage (BGSAVE, KEYS)
          After=valkey-server.service
          OnFailure=failure-notification@%n.service

          [Service]
          Type=oneshot
          RemainAfterExit=no
          Environment="REDIS_HOST={{ lookup('env', 'REDIS_HOST') }}"
          Environment="REDIS_PORT={{ lookup('env', 'REDIS_PORT') | default(lookup('env', 'REDIS_PORT'), true) }}"
          Environment="REDIS_PASSWORD={{ lookup('env', 'REDIS_PASSWORD') }}"
          Environment="MSMTP_RCPTS={{ lookup('env', 'MSMTP_RCPTS') | default('security@forwardemail.net', true) }}"
          ExecStart=/usr/local/bin/monitor-redis-commands.sh

    # Create systemd timer for Redis command monitoring (every hour)
    - name: Create Redis command monitoring systemd timer
      copy:
        dest: /etc/systemd/system/redis-command-monitor.timer
        content: |
          [Unit]
          Description=Monitor Redis Command Usage Hourly

          [Timer]
          OnBootSec=5min
          OnUnitActiveSec=1h

          [Install]
          WantedBy=timers.target

    - name: Enable and start Redis command monitoring timer
      systemd:
        daemon_reload: true
        name: redis-command-monitor.timer
        enabled: true
        state: started

    # ============================================================================
    # POST-DEPLOYMENT INSTRUCTIONS
    # ============================================================================
    - name: Display manual restart instructions
      shell: |
        cat << 'EOF'

        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
        â•‘                                                                        â•‘
        â•‘                    âš ï¸  MANUAL RESTART REQUIRED âš ï¸                      â•‘
        â•‘                                                                        â•‘
        â•‘  Valkey configuration has been updated but the service was NOT        â•‘
        â•‘  automatically restarted to prevent disruption.                       â•‘
        â•‘                                                                        â•‘
        â•‘  To apply the new configuration, you must MANUALLY restart Valkey:    â•‘
        â•‘                                                                        â•‘
        â•‘    sudo systemctl restart valkey-server                               â•‘
        â•‘                                                                        â•‘
        â•‘  âš ï¸  WARNING: This will:                                              â•‘
        â•‘    â€¢ Disconnect all active Redis connections                          â•‘
        â•‘    â€¢ Trigger BGSAVE if data has changed (may take 2-3 minutes)        â•‘
        â•‘    â€¢ Cause brief service interruption                                 â•‘
        â•‘                                                                        â•‘
        â•‘  ðŸ’¡ RECOMMENDATION:                                                   â•‘
        â•‘    â€¢ Schedule restart during low-traffic period                       â•‘
        â•‘    â€¢ Notify team before restarting                                    â•‘
        â•‘    â€¢ Monitor logs: sudo journalctl -u valkey-server -f               â•‘
        â•‘                                                                        â•‘
        â•‘  To verify Valkey is running after restart:                           â•‘
        â•‘    redis-cli -p 6380 --tls -a "$REDIS_PASSWORD" PING                 â•‘
        â•‘                                                                        â•‘
        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        EOF
      changed_when: false
